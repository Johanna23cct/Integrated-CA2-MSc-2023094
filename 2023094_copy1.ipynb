{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60e3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/johanna23cct/integrated-CA2-MSc-2023094.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0494ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#git remote add origin https://github.com/johanna23cct/integrated-CA2-MSc-2023094.git\n",
    "#git branch -M main\n",
    "#git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac692706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Matplotlib in /usr/lib/python3/dist-packages (3.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990b285b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bokeh in /home/hduser/.local/lib/python3.10/site-packages (3.3.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/lib/python3/dist-packages (from bokeh) (9.0.1)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/lib/python3/dist-packages (from bokeh) (21.3)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/hduser/.local/lib/python3.10/site-packages (from bokeh) (1.24.3)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/lib/python3/dist-packages (from bokeh) (5.4.1)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /usr/lib/python3/dist-packages (from bokeh) (3.0.3)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /home/hduser/.local/lib/python3.10/site-packages (from bokeh) (2023.10.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/hduser/.local/lib/python3.10/site-packages (from bokeh) (2.0.3)\n",
      "Requirement already satisfied: contourpy>=1 in /home/hduser/.local/lib/python3.10/site-packages (from bokeh) (1.1.1)\n",
      "Requirement already satisfied: tornado>=5.1 in /usr/lib/python3/dist-packages (from bokeh) (6.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/hduser/.local/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.2->bokeh) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hduser/.local/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2->bokeh) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e2bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: skforecast in /home/hduser/.local/lib/python3.10/site-packages (0.10.1)\n",
      "Requirement already satisfied: joblib<1.4,>=1.1.0 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (1.3.2)\n",
      "Requirement already satisfied: pandas<2.1,>=1.2 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (2.0.3)\n",
      "Requirement already satisfied: optuna<3.3,>=2.10.0 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (3.2.0)\n",
      "Requirement already satisfied: numpy<1.26,>=1.20 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn<1.4,>=1.0 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (1.3.0)\n",
      "Requirement already satisfied: tqdm<4.66,>=4.57.0 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (4.65.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from optuna<3.3,>=2.10.0->skforecast) (21.3)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in /home/hduser/.local/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (0.10.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/hduser/.local/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (2.0.22)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from optuna<3.3,>=2.10.0->skforecast) (5.4.1)\n",
      "Requirement already satisfied: colorlog in /home/hduser/.local/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (6.7.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/hduser/.local/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hduser/.local/lib/python3.10/site-packages (from pandas<2.1,>=1.2->skforecast) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/hduser/.local/lib/python3.10/site-packages (from pandas<2.1,>=1.2->skforecast) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas<2.1,>=1.2->skforecast) (2022.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/lib/python3/dist-packages (from scikit-learn<1.4,>=1.0->skforecast) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/hduser/.local/lib/python3.10/site-packages (from scikit-learn<1.4,>=1.0->skforecast) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/hduser/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna<3.3,>=2.10.0->skforecast) (4.5.0)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna<3.3,>=2.10.0->skforecast) (1.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<2.1,>=1.2->skforecast) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/hduser/.local/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna<3.3,>=2.10.0->skforecast) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install skforecast --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b68e89f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plots\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "%matplotlib inline\n",
    "\n",
    "# Modeling and Forecasting\n",
    "# ==============================================================================\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\n",
    "#from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "# Spark\n",
    "# import SparkSession library \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Warnings configuration\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22943cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext HOW I CAN SHOW THAT ... SC  It is not working to me, in the first time!\n",
    "# that ok now, I know, is sc.master.  I was put only sc\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cc08cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName('data_projectTweets').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce48466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a customer dataframe by declaring the schema and passing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb2dc8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType().add(\"Ids\",\"integer\").add(\"Date\",\"string\").add(\"Flag\", \"string\").add(\"User\",'string').add(\"Text\", \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1a847a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the values\n",
    "df = spark.createDataFrame([\n",
    "        (1467810369, \"Mon apr 06 22:19:45 PDT 2009\", \"NO_Query\", \"_theSpecialOne\", \"@switchfoot http://twitpic.com/2y1zl - Awww, t...\"), \n",
    "        (1467810672, \"Mon Apr 06 22:19:49 PDT 2009\", \"NO_Query\", \"scotthamilton\", \"is upset that he can not update his Facebook b...\" ),\n",
    "        (1467811184, \"Mon Apr 06 22:19:57 PDT 2009\", \"NO_Query\", \"mattycus\", \"@Kenichan I dived many times for the ball. Man...\"),\n",
    "    ], ['Ids', 'Date', 'Flag', 'User', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b969191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Duplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5efceb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of row: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:===========================================================(2 + 0) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of distinct rows: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('Count of row: {0}' .format(df.count()))\n",
    "print('Count of distinct rows: {0}' .format(df.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb7eee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If these two numbers differ - you have rows that are exact copies of each other. We can drop these rows by using the .dropDuplicates(...) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bab22d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+--------------+--------------------+\n",
      "|       Ids|                Date|    Flag|          User|                Text|\n",
      "+----------+--------------------+--------+--------------+--------------------+\n",
      "|1467810369|Mon apr 06 22:19:...|NO_Query|_theSpecialOne|@switchfoot http:...|\n",
      "|1467810672|Mon Apr 06 22:19:...|NO_Query| scotthamilton|is upset that he ...|\n",
      "|1467811184|Mon Apr 06 22:19:...|NO_Query|      mattycus|@Kenichan I dived...|\n",
      "+----------+--------------------+--------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a09707ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42f31ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of ids: 3\n",
      "Count of distinct ids: 3\n"
     ]
    }
   ],
   "source": [
    "print('Count of ids: {0}'.format(df.count()))\n",
    "print('Count of distinct ids: {0}'.format(df.select([c for c in df.columns if c != 'id']).distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a030a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We still have one more duplicate. We will use the .dropDuplicates(...) but add the subset parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c21daaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+--------------+--------------------+\n",
      "|       Ids|                Date|    Flag|          User|                Text|\n",
      "+----------+--------------------+--------+--------------+--------------------+\n",
      "|1467810369|Mon apr 06 22:19:...|NO_Query|_theSpecialOne|@switchfoot http:...|\n",
      "|1467810672|Mon Apr 06 22:19:...|NO_Query| scotthamilton|is upset that he ...|\n",
      "|1467811184|Mon Apr 06 22:19:...|NO_Query|      mattycus|@Kenichan I dived...|\n",
      "+----------+--------------------+--------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates(subset=[c for c in df.columns if c != 'id'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b757f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate the total and distinct number of IDs in one step we can use the .agg(...) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e685c494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|count|distinct|\n",
      "+-----+--------+\n",
      "|    3|       3|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "df.agg(\n",
    "    fn.count('ids').alias('count'),\n",
    "    fn.countDistinct('ids').alias('distinct')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d85c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give each row a unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef6b4c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+--------------+--------------------+------+\n",
      "|       Ids|                Date|    Flag|          User|                Text|new_id|\n",
      "+----------+--------------------+--------+--------------+--------------------+------+\n",
      "|1467810369|Mon apr 06 22:19:...|NO_Query|_theSpecialOne|@switchfoot http:...|     0|\n",
      "|1467810672|Mon Apr 06 22:19:...|NO_Query| scotthamilton|is upset that he ...|     1|\n",
      "|1467811184|Mon Apr 06 22:19:...|NO_Query|      mattycus|@Kenichan I dived...|     2|\n",
      "+----------+--------------------+--------+--------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_id', fn.monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b548e663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ids: long (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Flag: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the structure of Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27c55502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+--------------+--------------------+\n",
      "|       Ids|                Date|    Flag|          User|                Text|\n",
      "+----------+--------------------+--------+--------------+--------------------+\n",
      "|1467810369|Mon apr 06 22:19:...|NO_Query|_theSpecialOne|@switchfoot http:...|\n",
      "|1467810672|Mon Apr 06 22:19:...|NO_Query| scotthamilton|is upset that he ...|\n",
      "|1467811184|Mon Apr 06 22:19:...|NO_Query|      mattycus|@Kenichan I dived...|\n",
      "+----------+--------------------+--------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "493a4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with nukll values \n",
    "df_na = spark.createDataFrame([(1467810369, \"Mon apr 06 22:19:45 PDT 2009\", None, \"_theSpecialOne\", \"@switchfoot http://twitpic.com/2y1zl - Awww, t...\"), (1467810672, \"Mon Apr 06 22:19:49 PDT 2009\", None, \"scotthamilton\", \"is upset that he can not update his Facebook b...\" ), (1467811184, \"Mon Apr 06 22:19:57 PDT 2009\", None, \"mattycus\",\"@Kenichan I dived many times for the ball. Man...\")],schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78b906be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----+--------------+--------------------+\n",
      "|       Ids|                Date|Flag|          User|                Text|\n",
      "+----------+--------------------+----+--------------+--------------------+\n",
      "|1467810369|Mon apr 06 22:19:...|null|_theSpecialOne|@switchfoot http:...|\n",
      "|1467810672|Mon Apr 06 22:19:...|null| scotthamilton|is upset that he ...|\n",
      "|1467811184|Mon Apr 06 22:19:...|null|      mattycus|@Kenichan I dived...|\n",
      "+----------+--------------------+----+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b68e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----+--------------+--------------------+\n",
      "|       Ids|                Date|Flag|          User|                Text|\n",
      "+----------+--------------------+----+--------------+--------------------+\n",
      "|1467810369|Mon apr 06 22:19:...|   0|_theSpecialOne|@switchfoot http:...|\n",
      "|1467810672|Mon Apr 06 22:19:...|   0| scotthamilton|is upset that he ...|\n",
      "|1467811184|Mon Apr 06 22:19:...|   0|      mattycus|@Kenichan I dived...|\n",
      "+----------+--------------------+----+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#+---+------+------+----+------+\n",
    "#| id|weight|height| age|gender|\n",
    "# fill all null values with 0\n",
    "df_na.fillna('0').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7529d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----+--------------+--------------------+\n",
      "|       Ids|                Date|Flag|          User|                Text|\n",
      "+----------+--------------------+----+--------------+--------------------+\n",
      "|1467810369|Mon apr 06 22:19:...|null|_theSpecialOne|@switchfoot http:...|\n",
      "|1467810672|Mon Apr 06 22:19:...|null| scotthamilton|is upset that he ...|\n",
      "|1467811184|Mon Apr 06 22:19:...|null|      mattycus|@Kenichan I dived...|\n",
      "+----------+--------------------+----+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill null values with specific value\n",
    "df_na.fillna( { 'Ids':'1467811372', 'User':'elleCTF' } ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "988a8e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+----+\n",
      "|Ids|Date|Flag|User|Text|\n",
      "+---+----+----+----+----+\n",
      "+---+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Return new df omitting rows with null values\n",
    "df_na.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fb6ef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+----+\n",
      "|Ids|Date|Flag|User|Text|\n",
      "+---+----+----+----+----+\n",
      "+---+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na.na.drop(subset='Flag').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78da9cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----+--------------+--------------------+\n",
      "|       Ids|                Date|Flag|          User|                Text|\n",
      "+----------+--------------------+----+--------------+--------------------+\n",
      "|1467810369|Mon apr 06 22:19:...|null|_theSpecialOne|@switchfoot http:...|\n",
      "|1467810672|Mon Apr 06 22:19:...|null| scotthamilton|is upset that he ...|\n",
      "|1467811184|Mon Apr 06 22:19:...|null|      mattycus|@Kenichan I dived...|\n",
      "+----------+--------------------+----+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na.replace(\"0\",\"No Query\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c45feec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+--------------------+\n",
      "|       Ids|                Date|          User|                Text|\n",
      "+----------+--------------------+--------------+--------------------+\n",
      "|1467810369|Mon apr 06 22:19:...|_theSpecialOne|@switchfoot http:...|\n",
      "|1467810672|Mon Apr 06 22:19:...| scotthamilton|is upset that he ...|\n",
      "|1467811184|Mon Apr 06 22:19:...|      mattycus|@Kenichan I dived...|\n",
      "+----------+--------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#deleting column \n",
    "df.drop('Flag').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "217df978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "## Understand your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdb47f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# big headache and fever to read the dataset\n",
    "#    (command)+(file://)+(path)+(filename)\n",
    "\n",
    "#df = spark.read.csv('/home/hduser/Documents/CA2', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02317436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folowiong you can see the two way to load the dataset, \n",
    "#I will use direct from the path, to have one of five V' (Velocity) \n",
    "\n",
    "path = \"/user1/people.json\"\n",
    "#df = spark.read.csv(\"file:///home/hduser/Documents/CA2/ProjectTweets.csv\", header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8da8c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data. i can't belive, it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd24a29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweets = sc.textFile(\"file:///home/hduser/Documents/CA2/ProjectTweets.csv\")\n",
    "header = tweets.first()\n",
    "\n",
    "tweets = tweets \\\n",
    "    .filter(lambda row: row != header) \\\n",
    "    .map(lambda row: [int(elem) for elem in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f73e960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the schema for my DataFrame\n",
    "# was one  error because i didn' import pyspark.sql.types as typ\n",
    "# but next did it, the command worked, great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05e8145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    *[\n",
    "        typ.StructField(h[1:-1], typ.IntegerType(), True)\n",
    "        for h in header.split(',')\n",
    "    ]\n",
    "]\n",
    "schema = typ.StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "958e899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names  = ['Ids', 'Date', 'Flag', 'User', 'Text']    \n",
    "full_df = pd.read_csv('file:///home/hduser/Documents/CA2/ProjectTweets.csv', header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c77733de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ids</th>\n",
       "      <th>Date</th>\n",
       "      <th>Flag</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Ids                          Date      Flag             User  \\\n",
       "0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                Text  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b64a2555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43c94e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ids     1600000\n",
       "Date    1600000\n",
       "Flag    1600000\n",
       "User    1600000\n",
       "Text    1600000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of records\n",
    "full_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc43f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating my Dataframe (i did it in line 27 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "545c0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_df = spark.createDataFrame(tweets, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59b3f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6108c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For categorical columns we will count the frequencies of their values using .groupby(...) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cfaf428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_df.groupby('46781036457').count().show()  ***not working ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "934591b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# well, lest go to make  the basic descriptive statistics for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "600c9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the truly numerical features we can use the .describe() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6463135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in line 11 import SparkSession from PySpark and definned Spark =SparkSession.....\n",
    "#in line 13 defined the schema INT and String\n",
    "#in line 30 Lod Dataset. CSV.. df= spark.read.csv\n",
    "# inline 27 Display the structure of Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e14644c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Procesising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0f7a55f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"text\" among (0, 1467810369, Mon Apr 06 22:19:45 PDT 2009, NO_QUERY, _TheSpecialOne_, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6681/3015877286.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[^a-z0-9\\\\s]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1634\u001b[0m         \"\"\"\n\u001b[1;32m   1635\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1637\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"text\" among (0, 1467810369, Mon Apr 06 22:19:45 PDT 2009, NO_QUERY, _TheSpecialOne_, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D)"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Text\", lower(df[\"text\"]))\n",
    "df = df.withColumn(\"Text\", regexp_replace(df[\"text\"], \"[^a-z0-9\\\\s]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42db868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4de8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1276b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c2580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ff6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6680cd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ff69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e2c4e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd8298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
