{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60e3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/johanna23cct/integrated-CA2-MSc-2023094.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0494ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#git remote add origin https://github.com/johanna23cct/integrated-CA2-MSc-2023094.git\n",
    "#git branch -M main\n",
    "#git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0af54a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /usr/local/spark-3.2.4-bin-hadoop3.2/python (3.2.4)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/hduser/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d097472e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /home/hduser/.local/lib/python3.10/site-packages (2.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac692706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Matplotlib in /usr/lib/python3/dist-packages (3.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990b285b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bokeh in /home/hduser/.local/lib/python3.10/site-packages (3.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/lib/python3/dist-packages (from bokeh) (5.4.1)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /home/hduser/.local/lib/python3.10/site-packages (from bokeh) (2023.10.1)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/lib/python3/dist-packages (from bokeh) (21.3)\n",
      "Requirement already satisfied: tornado>=5.1 in /usr/lib/python3/dist-packages (from bokeh) (6.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/hduser/.local/lib/python3.10/site-packages (from bokeh) (1.24.3)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /usr/lib/python3/dist-packages (from bokeh) (3.0.3)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/lib/python3/dist-packages (from bokeh) (9.0.1)\n",
      "Requirement already satisfied: contourpy>=1 in /home/hduser/.local/lib/python3.10/site-packages (from bokeh) (1.1.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/hduser/.local/lib/python3.10/site-packages (from bokeh) (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hduser/.local/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.2->bokeh) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/hduser/.local/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2->bokeh) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96e2bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: skforecast in /home/hduser/.local/lib/python3.10/site-packages (0.10.1)\n",
      "Requirement already satisfied: optuna<3.3,>=2.10.0 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (3.2.0)\n",
      "Requirement already satisfied: numpy<1.26,>=1.20 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn<1.4,>=1.0 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (1.3.0)\n",
      "Requirement already satisfied: pandas<2.1,>=1.2 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (2.0.3)\n",
      "Requirement already satisfied: tqdm<4.66,>=4.57.0 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (4.65.2)\n",
      "Requirement already satisfied: joblib<1.4,>=1.1.0 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (1.3.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/hduser/.local/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (2.0.22)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from optuna<3.3,>=2.10.0->skforecast) (5.4.1)\n",
      "Requirement already satisfied: colorlog in /home/hduser/.local/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (6.7.0)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in /home/hduser/.local/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (0.10.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/hduser/.local/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (1.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from optuna<3.3,>=2.10.0->skforecast) (21.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/hduser/.local/lib/python3.10/site-packages (from pandas<2.1,>=1.2->skforecast) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hduser/.local/lib/python3.10/site-packages (from pandas<2.1,>=1.2->skforecast) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas<2.1,>=1.2->skforecast) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/hduser/.local/lib/python3.10/site-packages (from scikit-learn<1.4,>=1.0->skforecast) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/lib/python3/dist-packages (from scikit-learn<1.4,>=1.0->skforecast) (1.8.0)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna<3.3,>=2.10.0->skforecast) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/hduser/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna<3.3,>=2.10.0->skforecast) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<2.1,>=1.2->skforecast) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/hduser/.local/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna<3.3,>=2.10.0->skforecast) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install skforecast --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df842060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Tensorflow in /home/hduser/.local/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (3.9.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (1.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from Tensorflow) (1.16.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (0.4.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (16.0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (4.5.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (23.5.26)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (0.33.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (1.57.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (2.8.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (4.24.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/hduser/.local/lib/python3.10/site-packages (from Tensorflow) (2.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from Tensorflow) (59.6.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->Tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/hduser/.local/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->Tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/hduser/.local/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->Tensorflow) (2.3.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/hduser/.local/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->Tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/hduser/.local/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->Tensorflow) (2.22.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/hduser/.local/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->Tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.9,>=2.8->Tensorflow) (2.25.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/hduser/.local/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->Tensorflow) (3.4.4)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->Tensorflow) (1.26.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/hduser/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->Tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/hduser/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->Tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/hduser/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->Tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/hduser/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->Tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/hduser/.local/lib/python3.10/site-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->Tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/hduser/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->Tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->Tensorflow) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02cb1042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/hduser/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /home/hduser/.local/lib/python3.10/site-packages (from nltk) (4.65.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/hduser/.local/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/hduser/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbd2e428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in /home/hduser/.local/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/hduser/.local/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: joblib in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.65.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b68e89f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Plots\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "%matplotlib inline\n",
    "\n",
    "# Modeling and Forecasting\n",
    "# ==============================================================================\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\n",
    "#from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "347a8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3546d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.layers import Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d1d3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spark\n",
    "# import SparkSession library \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, col, lower, concat_ws, regexp_replace\n",
    "from pyspark.sql.functions import split, size, length, broadcast, sum\n",
    "from pyspark.sql.functions import to_timestamp, concat, lit, substring, when, month\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField, ArrayType\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Warnings configuration\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efeff729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22943cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext \n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce48466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a SparkSession  **from tutorial 4 - NN using Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cc08cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"MySparkApplication\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb2dc8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Ids\", IntegerType(), True),\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Flag\", StringType(), True),\n",
    "    StructField(\"User\", StringType(), True),\n",
    "    StructField(\"Text\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "217df978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************************************************************************************************************\n",
    "# *                                  Start with the Data                                                         *\n",
    "# ****************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee728360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big headache and colapse, to read the dataset\n",
    "# I was load the file from my hadoop (local)\n",
    "#spark_df = spark.read.csv(\"file:///home/hduser/Documents/CA2/ProjectTweets.csv\", header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02317436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move the dataset fiel to HDFS path from my terminal:\n",
    "#First check:  $fs -ls/user1  from /home/hduser/Documents/CA2/\n",
    "#Next move :   $hadoop fs-put./ProjectTweets.scv/user1\n",
    "#Check again : $fs -ls/user1\n",
    "#I will use direct from the path, to have one of five V' (Velocity) \n",
    "\n",
    "\n",
    "#folowiong you can see the two way to load the dataset, \n",
    "#    (command)+(file://)+(/path/)+(filename)\n",
    "#I will use direct from the path, to have one of five V' (Velocity) \n",
    "\n",
    "#Tweets_path = \"/user1/ProjectTweets.csv\"\n",
    "#df = spark.read.csv(\"file:///home/hduser/Documents/CA2/ProjectTweets.csv\", header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b200bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.csv(\"/user1/ProjectTweets.csv\", header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "343bec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90864784",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['_c0', 'Ids', 'Date', 'Flag', 'User', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43d21cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, column_name in enumerate(column_names):\n",
    "    spark_df = spark_df.withColumnRenamed(\"_c\" + str(i),column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "084011e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       Ids|                Date|    Flag|           User|                Text|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Showing the data\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2db9aba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# display the total number of rows data\n",
    "total_rows = spark_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e283b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 1,600,000\n"
     ]
    }
   ],
   "source": [
    "formatted_total_rows = \"{:,}\".format(total_rows)\n",
    "print(\"Total Rows:\", formatted_total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b74b848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary column  ************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "840bac8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_to_drop = ['Flag', 'Unnamed: 0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e99b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.drop('Flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2fda977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+---------------+--------------------+\n",
      "|_c0|       Ids|                Date|           User|                Text|\n",
      "+---+----------+--------------------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes |\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70531f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a UDF to apply VADER sentiment analysis to a Text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "566ba00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = (\n",
    "    StructType()\n",
    "    .add(\"_c0\",\"integer\")\n",
    "    .add(\"Ids\",\"integer\")\n",
    "    .add(\"Date\",\"string\")\n",
    "    .add(\"Flag\", \"string\")\n",
    "    .add(\"User\",\"string\")\n",
    "    .add(\"Text\", \"string\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b38b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(Text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment = sid.polarity_scores(Text)\n",
    "    return sentiment['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a97aafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#register the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b98f4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment_udf = udf(lambda text:spark_df(\"Text\"), StringType()) \n",
    "sentiment_udf = udf(analyze_sentiment, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "786eb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a43e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"analyze_sentiment\", sentiment_udf(spark_df[\"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3311372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(analyze_sentiment.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7ea7db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|                Text|analyze_sentiment|\n",
      "+--------------------+-----------------+\n",
      "|@switchfoot http:...|          -0.0173|\n",
      "|is upset that he ...|            -0.75|\n",
      "|@Kenichan I dived...|           0.4939|\n",
      "|my whole body fee...|            -0.25|\n",
      "|@nationwideclass ...|          -0.6597|\n",
      "|@Kwesidei not the...|              0.0|\n",
      "|         Need a hug |           0.4767|\n",
      "|@LOLTrish hey  lo...|            0.745|\n",
      "|@Tatiana_K nope t...|              0.0|\n",
      "|@twittera que me ...|              0.0|\n",
      "|spring break in p...|              0.0|\n",
      "|I just re-pierced...|              0.0|\n",
      "|@caregiving I cou...|          -0.5994|\n",
      "|@octolinz16 It it...|          -0.1027|\n",
      "|@smarrison i woul...|           0.3724|\n",
      "|@iamjazzyfizzle I...|           0.4545|\n",
      "|Hollis' death sce...|          -0.9081|\n",
      "|about to file taxes |              0.0|\n",
      "|@LettyA ahh ive a...|           0.6988|\n",
      "|@FakerPattyPattz ...|           0.1779|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 14:09:10,465 WARN python.PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 7 (TID 8): Attempting to kill Python Worker\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.select(\"Text\", \"analyze_sentiment\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68d7fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'Ids', 'Date', 'Flag', 'User', 'Text']\n"
     ]
    }
   ],
   "source": [
    "#why i have Flag  again??\n",
    "print(column_names[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10198758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+---------------+--------------------+-----------------+\n",
      "|_c0|       Ids|                Date|           User|                Text|analyze_sentiment|\n",
      "+---+----------+--------------------+---------------+--------------------+-----------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|          -0.0173|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|            -0.75|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|           0.4939|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|            -0.25|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|          -0.6597|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|              0.0|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |           0.4767|\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|            0.745|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|              0.0|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|              0.0|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|              0.0|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|              0.0|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|          -0.5994|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|          -0.1027|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|           0.3724|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|           0.4545|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|          -0.9081|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes |              0.0|\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|           0.6988|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|           0.1779|\n",
      "+---+----------+--------------------+---------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#to show\n",
    "\n",
    "spark_df.drop('Flag').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fd7df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(spark_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d79e15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#        Basic Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e758298",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"word_count\", size(split(spark_df[\"Text\"], \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec1d7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6da983c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                Text|word_count|\n",
      "+--------------------+----------+\n",
      "|@switchfoot http:...|        20|\n",
      "|is upset that he ...|        22|\n",
      "|@Kenichan I dived...|        19|\n",
      "|my whole body fee...|        11|\n",
      "|@nationwideclass ...|        22|\n",
      "|@Kwesidei not the...|         6|\n",
      "|         Need a hug |         4|\n",
      "|@LOLTrish hey  lo...|        24|\n",
      "|@Tatiana_K nope t...|         7|\n",
      "|@twittera que me ...|         6|\n",
      "|spring break in p...|         8|\n",
      "|I just re-pierced...|         6|\n",
      "|@caregiving I cou...|        21|\n",
      "|@octolinz16 It it...|        16|\n",
      "|@smarrison i woul...|        23|\n",
      "|@iamjazzyfizzle I...|        20|\n",
      "|Hollis' death sce...|        19|\n",
      "|about to file taxes |         5|\n",
      "|@LettyA ahh ive a...|        12|\n",
      "|@FakerPattyPattz ...|        13|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(\"Text\", \"word_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5312f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tutorial did't work because is a Panda code: TypeError: 'Column' object is not callable\n",
    "\n",
    "#spark_df ['word_count'] = spark_df ['Text'].apply(lambda x: len(str(x).split(\"\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86e96ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df[['Text','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f000ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44bfbfc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tutorial did't work because is a Panda code:TypeError: 'Column' object is not callable\n",
    "#Tweets['char_count'] = Tweets['Text'].str.len()\n",
    "## this also includes spaces\n",
    "#Tweets[['Text','char_count']].head()\n",
    "#Tweets = Tweets.withColumn(\"char_count\", length(Tweets[\"Text\"]))\n",
    "#Tweets.select(\"Text\", \"char_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "33b87218",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"char_count\", length(spark_df[\"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1bc221ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                Text|char_count|\n",
      "+--------------------+----------+\n",
      "|@switchfoot http:...|       115|\n",
      "|is upset that he ...|       111|\n",
      "|@Kenichan I dived...|        89|\n",
      "|my whole body fee...|        47|\n",
      "|@nationwideclass ...|       111|\n",
      "|@Kwesidei not the...|        29|\n",
      "|         Need a hug |        11|\n",
      "|@LOLTrish hey  lo...|        99|\n",
      "|@Tatiana_K nope t...|        36|\n",
      "|@twittera que me ...|        25|\n",
      "|spring break in p...|        43|\n",
      "|I just re-pierced...|        26|\n",
      "|@caregiving I cou...|        94|\n",
      "|@octolinz16 It it...|        77|\n",
      "|@smarrison i woul...|       117|\n",
      "|@iamjazzyfizzle I...|       103|\n",
      "|Hollis' death sce...|        93|\n",
      "|about to file taxes |        20|\n",
      "|@LettyA ahh ive a...|        64|\n",
      "|@FakerPattyPattz ...|        79|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(\"Text\", \"char_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2664f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27f18d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"Text\", concat_ws(\"\", lower(col(\"Text\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3f5ed83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Text|\n",
      "+--------------------+\n",
      "|@switchfoot http:...|\n",
      "|is upset that he ...|\n",
      "|@kenichan i dived...|\n",
      "|my whole body fee...|\n",
      "|@nationwideclass ...|\n",
      "|@kwesidei not the...|\n",
      "|         need a hug |\n",
      "|@loltrish hey  lo...|\n",
      "|@tatiana_k nope t...|\n",
      "|@twittera que me ...|\n",
      "|spring break in p...|\n",
      "|i just re-pierced...|\n",
      "|@caregiving i cou...|\n",
      "|@octolinz16 it it...|\n",
      "|@smarrison i woul...|\n",
      "|@iamjazzyfizzle i...|\n",
      "|hollis' death sce...|\n",
      "|about to file taxes |\n",
      "|@lettya ahh ive a...|\n",
      "|@fakerpattypattz ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(\"Text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a865830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number  of Special characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b1e6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName(\"SpecialCharactersCount\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "270fe5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Special_Characters = [\"@\", \"#\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b26bcd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_Special_Characters(text):\n",
    "    count = 0\n",
    "    for char in Special_Characters:\n",
    "        count += text.count(char)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c61ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_Special_Characters_udf = udf(count_Special_Characters, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9bb7ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"Special_Characters_count\", count_Special_Characters_udf(col(\"Text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa22f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = spark_df.select(\"Text\", \"Special_Characters_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80ea2cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+\n",
      "|                Text|Special_Characters_count|\n",
      "+--------------------+------------------------+\n",
      "|@switchfoot http:...|                       1|\n",
      "|is upset that he ...|                       0|\n",
      "|@kenichan i dived...|                       1|\n",
      "|my whole body fee...|                       0|\n",
      "|@nationwideclass ...|                       1|\n",
      "|@kwesidei not the...|                       1|\n",
      "|         need a hug |                       0|\n",
      "|@loltrish hey  lo...|                       1|\n",
      "|@tatiana_k nope t...|                       1|\n",
      "|@twittera que me ...|                       1|\n",
      "|spring break in p...|                       0|\n",
      "|i just re-pierced...|                       0|\n",
      "|@caregiving i cou...|                       1|\n",
      "|@octolinz16 it it...|                       1|\n",
      "|@smarrison i woul...|                       1|\n",
      "|@iamjazzyfizzle i...|                       2|\n",
      "|hollis' death sce...|                       0|\n",
      "|about to file taxes |                       0|\n",
      "|@lettya ahh ive a...|                       1|\n",
      "|@fakerpattypattz ...|                       1|\n",
      "+--------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03d3c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b0ef5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"Text\", regexp_replace(col(\"Text\"), r'[^\\w\\s]', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e490786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Text|\n",
      "+--------------------+\n",
      "|switchfoot httptw...|\n",
      "|is upset that he ...|\n",
      "|kenichan i dived ...|\n",
      "|my whole body fee...|\n",
      "|nationwideclass n...|\n",
      "|kwesidei not the ...|\n",
      "|         need a hug |\n",
      "|loltrish hey  lon...|\n",
      "|tatiana_k nope th...|\n",
      "|twittera que me m...|\n",
      "|spring break in p...|\n",
      "|i just repierced ...|\n",
      "|caregiving i coul...|\n",
      "|octolinz16 it it ...|\n",
      "|smarrison i would...|\n",
      "|iamjazzyfizzle i ...|\n",
      "|hollis death scen...|\n",
      "|about to file taxes |\n",
      "|lettya ahh ive al...|\n",
      "|fakerpattypattz o...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(\"Text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "56a747e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count stop Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9c07085",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"is\", \"the\", \"I\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "539f3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stopwords(Text):\n",
    "    words = Text.split()\n",
    "#   return len([word for word in words if word in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7cb76ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_stopwords_udf = udf(count_stopwords, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "68b5a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"Stopwords\", count_stopwords_udf(col(\"Text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2dde6366",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                Text|Stopwords|\n",
      "+--------------------+---------+\n",
      "|switchfoot httptw...|     null|\n",
      "|is upset that he ...|     null|\n",
      "|kenichan i dived ...|     null|\n",
      "|my whole body fee...|     null|\n",
      "|nationwideclass n...|     null|\n",
      "|kwesidei not the ...|     null|\n",
      "|         need a hug |     null|\n",
      "|loltrish hey  lon...|     null|\n",
      "|tatiana_k nope th...|     null|\n",
      "|twittera que me m...|     null|\n",
      "|spring break in p...|     null|\n",
      "|i just repierced ...|     null|\n",
      "|caregiving i coul...|     null|\n",
      "|octolinz16 it it ...|     null|\n",
      "|smarrison i would...|     null|\n",
      "|iamjazzyfizzle i ...|     null|\n",
      "|hollis death scen...|     null|\n",
      "|about to file taxes |     null|\n",
      "|lettya ahh ive al...|     null|\n",
      "|fakerpattypattz o...|     null|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#spark_df.select(\"Text\", \"Sentiment\", \"Stopwords\").show()\n",
    "#spark_df.show(8)\n",
    "spark_df.select(\"Text\", \"Stopwords\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3e7ca445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************************************************************************\n",
    "#*                                       T O K E N I Z A T I O N                                                 *\n",
    "#*****************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4ed5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName(\"Tokenization\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d922c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df = spark.read.csv(\"/user1/ProjectTweets.csv\", header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8cd4ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into input (x) and output (y) variables\n",
    "#x = spark_df[:.0:5]\n",
    "#y = spark_df[:,5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a4a2b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model\n",
    "#model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663becd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ea40a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to Tokenization using TextBlob (2.7 Tokenization)\n",
    "#def tokenize_spark_df(Text):\n",
    "#    blob = TextBlob(Text)\n",
    "#    return list(blob.words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5e9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register\n",
    "#tokenize_udf = udf(tokenize_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "23c64a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply teh UDF\n",
    "#spark_df = spark_df.withColumn('Words', tokenize_udf(spark_df['_c5']))\n",
    "#spark_df = spark_df.withColumn('tokenized_Tweets', tokenize_udf(spark_df['_c5']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9b460189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df.select('Words').show()\n",
    "#spark_df[['Text', 'tokenized_Tweets']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4af13c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################nothing run###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "84a2bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#following funtion to get sentiment polarity using TextBlob\n",
    "def get_sentiment(Text):\n",
    "    blob = TextBlob(Text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e99d0707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register the UDF\n",
    "sentiment_udf = udf(get_sentiment, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17c83b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df[\"sentiment\"]=spark_df\"[\"Text\"].apply(lambda x: Textblob(x).sentimet[0])\n",
    "spark_df = spark_df.withColumn('sentiment', sentiment_udf(spark_df['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c47460a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                Text|  sentiment|\n",
      "+--------------------+-----------+\n",
      "|switchfoot httptw...|        0.2|\n",
      "|is upset that he ...|        0.0|\n",
      "|kenichan i dived ...|        0.5|\n",
      "|my whole body fee...|        0.2|\n",
      "|nationwideclass n...|     -0.625|\n",
      "|kwesidei not the ...|        0.2|\n",
      "|         need a hug |        0.0|\n",
      "|loltrish hey  lon...| 0.27333334|\n",
      "|tatiana_k nope th...|        0.0|\n",
      "|twittera que me m...|        0.0|\n",
      "|spring break in p...|-0.21428572|\n",
      "|i just repierced ...|        0.0|\n",
      "|caregiving i coul...|        0.0|\n",
      "|octolinz16 it it ...|        0.0|\n",
      "|smarrison i would...|      0.075|\n",
      "|iamjazzyfizzle i ...|        0.0|\n",
      "|hollis death scen...|        0.0|\n",
      "|about to file taxes |        0.0|\n",
      "|lettya ahh ive al...|        0.5|\n",
      "|fakerpattypattz o...|        0.0|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.select(\"Text\", \"sentiment\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5c175a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize sentiment\n",
    "def categorize_sentiment(score):\n",
    "    if score > 0:\n",
    "        return 'Positive'\n",
    "    elif score < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c7024b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply function UDF\n",
    "categorize_udf = udf(categorize_sentiment, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bd615e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the score\n",
    "spark_df = spark_df.withColumn('sentiment_category', categorize_udf(spark_df['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "773f64f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                Text|sentiment_category|\n",
      "+--------------------+------------------+\n",
      "|switchfoot httptw...|          Positive|\n",
      "|is upset that he ...|           Neutral|\n",
      "|kenichan i dived ...|          Positive|\n",
      "|my whole body fee...|          Positive|\n",
      "|nationwideclass n...|          Negative|\n",
      "|kwesidei not the ...|          Positive|\n",
      "|         need a hug |           Neutral|\n",
      "|loltrish hey  lon...|          Positive|\n",
      "|tatiana_k nope th...|           Neutral|\n",
      "|twittera que me m...|           Neutral|\n",
      "|spring break in p...|          Negative|\n",
      "|i just repierced ...|           Neutral|\n",
      "|caregiving i coul...|           Neutral|\n",
      "|octolinz16 it it ...|           Neutral|\n",
      "|smarrison i would...|          Positive|\n",
      "|iamjazzyfizzle i ...|           Neutral|\n",
      "|hollis death scen...|           Neutral|\n",
      "|about to file taxes |           Neutral|\n",
      "|lettya ahh ive al...|          Positive|\n",
      "|fakerpattypattz o...|           Neutral|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#show the categorized sentiment\n",
    "spark_df.select('Text', 'sentiment_category').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b4755d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8d7cbbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+-----------+------------------+\n",
      "|                Text|                Date|analyze_sentiment|  sentiment|sentiment_category|\n",
      "+--------------------+--------------------+-----------------+-----------+------------------+\n",
      "|switchfoot httptw...|Mon Apr 06 22:19:...|          -0.0173|        0.2|          Positive|\n",
      "|is upset that he ...|Mon Apr 06 22:19:...|            -0.75|        0.0|           Neutral|\n",
      "|kenichan i dived ...|Mon Apr 06 22:19:...|           0.4939|        0.5|          Positive|\n",
      "|my whole body fee...|Mon Apr 06 22:19:...|            -0.25|        0.2|          Positive|\n",
      "|nationwideclass n...|Mon Apr 06 22:19:...|          -0.6597|     -0.625|          Negative|\n",
      "|kwesidei not the ...|Mon Apr 06 22:20:...|              0.0|        0.2|          Positive|\n",
      "|         need a hug |Mon Apr 06 22:20:...|           0.4767|        0.0|           Neutral|\n",
      "|loltrish hey  lon...|Mon Apr 06 22:20:...|            0.745| 0.27333334|          Positive|\n",
      "|tatiana_k nope th...|Mon Apr 06 22:20:...|              0.0|        0.0|           Neutral|\n",
      "|twittera que me m...|Mon Apr 06 22:20:...|              0.0|        0.0|           Neutral|\n",
      "|spring break in p...|Mon Apr 06 22:20:...|              0.0|-0.21428572|          Negative|\n",
      "|i just repierced ...|Mon Apr 06 22:20:...|              0.0|        0.0|           Neutral|\n",
      "|caregiving i coul...|Mon Apr 06 22:20:...|          -0.5994|        0.0|           Neutral|\n",
      "|octolinz16 it it ...|Mon Apr 06 22:20:...|          -0.1027|        0.0|           Neutral|\n",
      "|smarrison i would...|Mon Apr 06 22:20:...|           0.3724|      0.075|          Positive|\n",
      "|iamjazzyfizzle i ...|Mon Apr 06 22:20:...|           0.4545|        0.0|           Neutral|\n",
      "|hollis death scen...|Mon Apr 06 22:20:...|          -0.9081|        0.0|           Neutral|\n",
      "|about to file taxes |Mon Apr 06 22:20:...|              0.0|        0.0|           Neutral|\n",
      "|lettya ahh ive al...|Mon Apr 06 22:20:...|           0.6988|        0.5|          Positive|\n",
      "|fakerpattypattz o...|Mon Apr 06 22:20:...|           0.1779|        0.0|           Neutral|\n",
      "+--------------------+--------------------+-----------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#spark_df.show()\n",
    "spark_df.select('Text', 'Date', 'analyze_sentiment', 'sentiment', 'sentiment_category').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b692ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the 'Date' column and creating new columns for day, month, and year\n",
    "spark_df = spark_df.withColumn('Day', substring('Date', 9, 3))\n",
    "spark_df = spark_df.withColumn('Month', substring('Date', 5, 2))\n",
    "spark_df = spark_df.withColumn('Year', substring('Date', -4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "50af7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting month abbreviation to numeric representation using 'when' statement\n",
    "spark_df = spark_df.withColumn('Month_Num',\n",
    "    when(col('Month') == 'Jan', '01')\n",
    "    .when(col('Month') == 'Feb', '02')\n",
    "    .when(col('Month') == 'Mar', '03')\n",
    "    .when(col('Month') == 'Apr', '04')\n",
    "    .when(col('Month') == 'May', '05')\n",
    "    .when(col('Month') == 'Jun', '06')\n",
    "    .when(col('Month') == 'Jul', '07')\n",
    "    .when(col('Month') == 'Aug', '08')\n",
    "    .when(col('Month') == 'Sep', '09')\n",
    "    .when(col('Month') == 'Oct', '10')\n",
    "    .when(col('Month') == 'Nov', '11')\n",
    "    .when(col('Month') == 'Dec', '12')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fa82b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new date column in the format 'yyyy-MM-dd HH:mm:ss' by concatenating the columns\n",
    "spark_df = spark_df.withColumn('New_Date',\n",
    "    concat(col('Year'), lit('-'), col('Month_Num'), lit('-'), col('Day'), lit(' '), substring('Date', 12, 8))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1268d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting month abbreviation to numeric representation\n",
    "month_dict = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',\n",
    "              'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "446865c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a funtion\n",
    "def map_month(month):\n",
    "    return month_dict.get(month, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2cb34be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a UDF\n",
    "map_month_udf = udf(map_month, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7c92f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting month to numeric representation\n",
    "#for month, month_num in month_dict.items():\n",
    "#    spark_df = spark_df.withColumn('Month_Num', when(col('Month') == month, month_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "13ead795",
   "metadata": {},
   "outputs": [],
   "source": [
    "for month, month_num in month_dict.items():\n",
    "    spark_df = spark_df.withColumn('Month_Num', when(col('Month') == month, month_num).otherwise(col('Month_Num')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "99b4ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new date column in the format 'yyyy-MM-dd HH:mm:ss' by concatenating the columns\n",
    "spark_df = spark_df.withColumn('New_Date',\n",
    "    concat(col('Year'), lit('-'), col('Month_Num'), lit('-'), col('Day'), lit(' '), substring('Date', 12, 8))\n",
    ")                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e5f4eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn('Month_Num', map_month_udf(col('Month')))\n",
    "\n",
    "#                               when(col('Month').isin(list(month_dict.keys())), \n",
    " #                                   month_dict[col('Month')]).otherwise(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1cfc34d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+------------------+---+-----+----+\n",
      "|                Text|                Date|  sentiment|sentiment_category|Day|Month|Year|\n",
      "+--------------------+--------------------+-----------+------------------+---+-----+----+\n",
      "|switchfoot httptw...|Mon Apr 06 22:19:...|        0.2|          Positive|06 |   Ap|2009|\n",
      "|is upset that he ...|Mon Apr 06 22:19:...|        0.0|           Neutral|06 |   Ap|2009|\n",
      "|kenichan i dived ...|Mon Apr 06 22:19:...|        0.5|          Positive|06 |   Ap|2009|\n",
      "|my whole body fee...|Mon Apr 06 22:19:...|        0.2|          Positive|06 |   Ap|2009|\n",
      "|nationwideclass n...|Mon Apr 06 22:19:...|     -0.625|          Negative|06 |   Ap|2009|\n",
      "|kwesidei not the ...|Mon Apr 06 22:20:...|        0.2|          Positive|06 |   Ap|2009|\n",
      "|         need a hug |Mon Apr 06 22:20:...|        0.0|           Neutral|06 |   Ap|2009|\n",
      "|loltrish hey  lon...|Mon Apr 06 22:20:...| 0.27333334|          Positive|06 |   Ap|2009|\n",
      "|tatiana_k nope th...|Mon Apr 06 22:20:...|        0.0|           Neutral|06 |   Ap|2009|\n",
      "|twittera que me m...|Mon Apr 06 22:20:...|        0.0|           Neutral|06 |   Ap|2009|\n",
      "|spring break in p...|Mon Apr 06 22:20:...|-0.21428572|          Negative|06 |   Ap|2009|\n",
      "|i just repierced ...|Mon Apr 06 22:20:...|        0.0|           Neutral|06 |   Ap|2009|\n",
      "|caregiving i coul...|Mon Apr 06 22:20:...|        0.0|           Neutral|06 |   Ap|2009|\n",
      "|octolinz16 it it ...|Mon Apr 06 22:20:...|        0.0|           Neutral|06 |   Ap|2009|\n",
      "|smarrison i would...|Mon Apr 06 22:20:...|      0.075|          Positive|06 |   Ap|2009|\n",
      "|iamjazzyfizzle i ...|Mon Apr 06 22:20:...|        0.0|           Neutral|06 |   Ap|2009|\n",
      "|hollis death scen...|Mon Apr 06 22:20:...|        0.0|           Neutral|06 |   Ap|2009|\n",
      "|about to file taxes |Mon Apr 06 22:20:...|        0.0|           Neutral|06 |   Ap|2009|\n",
      "|lettya ahh ive al...|Mon Apr 06 22:20:...|        0.5|          Positive|06 |   Ap|2009|\n",
      "|fakerpattypattz o...|Mon Apr 06 22:20:...|        0.0|           Neutral|06 |   Ap|2009|\n",
      "+--------------------+--------------------+-----------+------------------+---+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame with the new 'New_Date' column\n",
    "spark_df.select('Text', 'Date', 'sentiment', 'sentiment_category', 'Day', 'Month', 'Year').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b15a0cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 23:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "|Day|Month|Year|\n",
      "+---+-----+----+\n",
      "|21 |Ap   |2009|\n",
      "|24 |Ju   |2009|\n",
      "|19 |Ju   |2009|\n",
      "|06 |Ju   |2009|\n",
      "|03 |Ma   |2009|\n",
      "|11 |Ma   |2009|\n",
      "|18 |Ma   |2009|\n",
      "|28 |Ma   |2009|\n",
      "|27 |Ma   |2009|\n",
      "|02 |Ju   |2009|\n",
      "|02 |Ma   |2009|\n",
      "|13 |Ma   |2009|\n",
      "|09 |Ma   |2009|\n",
      "|26 |Ma   |2009|\n",
      "|17 |Ju   |2009|\n",
      "|20 |Ju   |2009|\n",
      "|21 |Ma   |2009|\n",
      "|17 |Ap   |2009|\n",
      "|20 |Ap   |2009|\n",
      "|31 |Ma   |2009|\n",
      "+---+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Date:show only the days 06= represent de day of the month\n",
    "spark_df.select('Day', 'Month', 'Year').distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3158890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat columns\n",
    "spark_df = spark_df.withColumn('Date', concat(col('Year'), col('Month'), col('Day')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d192c8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+---------+\n",
      "|Day|Month|Year|Date     |\n",
      "+---+-----+----+---------+\n",
      "|18 |Ap   |2009|2009Ap18 |\n",
      "|05 |Ju   |2009|2009Ju05 |\n",
      "|19 |Ap   |2009|2009Ap19 |\n",
      "|09 |Ma   |2009|2009Ma09 |\n",
      "|21 |Ap   |2009|2009Ap21 |\n",
      "|29 |Ma   |2009|2009Ma29 |\n",
      "|07 |Ju   |2009|2009Ju07 |\n",
      "|25 |Ma   |2009|2009Ma25 |\n",
      "|21 |Ma   |2009|2009Ma21 |\n",
      "|31 |Ma   |2009|2009Ma31 |\n",
      "|23 |Ju   |2009|2009Ju23 |\n",
      "|22 |Ju   |2009|2009Ju22 |\n",
      "|27 |Ma   |2009|2009Ma27 |\n",
      "|01 |Ma   |2009|2009Ma01 |\n",
      "|16 |Ma   |2009|2009Ma16 |\n",
      "|10 |Ma   |2009|2009Ma10 |\n",
      "|18 |Ma   |2009|2009Ma18 |\n",
      "|23 |Ma   |2009|2009Ma23 |\n",
      "|04 |Ju   |2009|2009Ju04 |\n",
      "|14 |Ju   |2009|2009Ju14 |\n",
      "+---+-----+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Date:new cariable\n",
    "spark_df.select('Day', 'Month', 'Year', 'Date').distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "df138346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the Data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb899d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0597c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c841b05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148bf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072243f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e023218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"Day\", substring('Date', 5, 2))\n",
    "spark_df = spark_df.withColumn(\"Month\", substring('Date', 9, 3))\n",
    "spark_df = spark_df.withColumn(\"Year\", substring('Date', -4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699ec321",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn('Month_Num',\n",
    "#    concat(col('Year'), lit('-'),\n",
    "               when(col('Month') == 'Jan', ('01')\n",
    "               .when(col('Month') == 'Feb', ('02') \n",
    "               .when(col('Month') == 'Mar', ('03') \n",
    "               .when(col('Month') == 'Apr', ('04') \n",
    "               .when(col('Month') == 'May', ('05') \n",
    "               .when(col('Month') == 'Jun', ('06') \n",
    "               .when(col('Month') == 'Jul', ('07') \n",
    "               .when(col('Month') == 'Aug', ('08') \n",
    "               .when(col('Month') == 'Sep', ('09') \n",
    "               .when(col('Month') == 'Oct', ('10') \n",
    "               .when(col('Month') == 'Nov', ('11') \n",
    "               .otherwise('12')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b07320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18af19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6532f556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9984d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073c205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59deb86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78145356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(date_string):\n",
    "    date_format = \"%a %b %d %H:%M:%S %Z %Y\"\n",
    "    try:\n",
    "        datetime_obj = datetime.strptime(date_string, date_format)\n",
    "        return datetime_obj\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply function UDF\n",
    "date_string_udf = udf(date_string, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spart_df = spark_df.withColumn('Converted_Date', col(Date).cast('timestamp').alias('Converted_Date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d91234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_string = \"Mon Apr 06 22:19:48 PDT 2009\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f616c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_Date = converted_Date(date_string)\n",
    "if converted_Date:\n",
    "    print(f\"Converted Date: {converted_Date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df['Converted date'] = spark_df['Date'].apply(convert_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c71e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c93813",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn('new_data', to_timestamp('date', 'E MMM dd HH:mm:ss zzz yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670cb1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f31bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81f0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb868ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b38b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08afbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data processing sentiment.....later   using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64466d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment_count = spark_df['Text'].value_counts()\n",
    "#sentiment_count = spark_df.groupBy['Text'].count().alias(\"count\")\n",
    "#sentiment_count = spark_df.groupBy['Text'].agg(count(\"*\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e108395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfef87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39f599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae35ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31041b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155f86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages Word ** Spark session **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a12a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName(\"AverageWordLength\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df95da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def avg_word(sentence):\n",
    "#    words = sentence.split()\n",
    "#    return sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_word_udf = udf(avg_word, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c9bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df.withColumn(\"average_word_length\", avg_word_udf(col(\"Sentiment\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a06e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_columns = spark_df.select(\"Sentiment\", \"average_word_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca27493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0340256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c20e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df = spark.createDataFrame(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2580bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def avg_word(sentence):\n",
    "#    words = sentence.split()\n",
    "#    return sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_word_udf = udf(avg_word, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets_df  = Tweets_df .withColumn(\"average_word_length\", avg_word_udf(col(\"_6\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dcba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287ebd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema for the  DataFrame\n",
    "#schema = StructType([StructField(\"sentence\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e609e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Crate a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f36b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets = [(\"Text\",)]\n",
    "#df = spark.createDataFrame(Tweets, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185526a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df =df.withColumn(\"words\", split(col(\"sentence\"), \" \"))\n",
    "#df =df.withColumn(\"words_lengths\", size(col(\"words\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8054f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul the average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_word_length = df.select(sum(col(\"word_lengths\")).alias(\"total_length\")).collect()[0][\"total_length\"] / df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Average word length: {avg_word_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439de19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName(\"AverageWordLength\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e630c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def avg_word(sentence):\n",
    "#    words = sentence.split()\n",
    "#    return sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616652b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_word_udf = udf(avg_word, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d5c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df = spark_df.withColumn(\"average_word_length\", avg_word_udf(col(\"Text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5659da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df [\"avg_word\"] = spark_df [\"Text\"].apply(lambda x : avg_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03810dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55626e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4cf802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_word_udf = udf(lambda sentence: avg_word(sentence), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879dfbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets = Tweets.withColumn(\"average_word_length\", avg_word_udf(col(\"Text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f392a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d68076a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b472d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e95ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c07ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafc41c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679e422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a063ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d38b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd965a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd407b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855bcd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec531d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af8b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f47f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f2eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22a48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9af6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd635b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef43ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77667d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf1ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9275b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f7ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d18f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b1124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90246c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************************************************************************************\n",
    "#*                                        S T R E M M I N G                                                     *\n",
    "#****************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a7c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Udf for Stremming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    words = nltk.word_tokenize(Text)\n",
    "    stemmer_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ff488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb79476",
   "metadata": {},
   "outputs": [],
   "source": [
    "strem_udf = udf(stem_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382451d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113bec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"StemmingExample\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fca207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_stemmed = spark_df.withColumn(\"Stemmed_Text\", strem_udf(spark_df[\"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show te resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f840a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_stemmed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a08e036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382efd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14644c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Procesising in panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d758ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_df = spark.read.csv(\"file:///home/hduser/Documents/CA2/Header_Proj_Tweets.csv\", header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_df = Tweets_df.withColumn(\"Text\", lower(Tweets_df[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42db868",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_df = Tweets_df.withColumn(\"Text\", regexp_replace(Tweets_df[\"text\"], \"[^a-z0-9\\\\s]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4de8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Tweets_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Ids', 'date', 'User', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4940e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot (Tweets_df[cols], heigt=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aad81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a068dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f828a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d8279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca469bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ff6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6680cd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ff69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Text\", lower(df[\"text\"]))\n",
    "df = df.withColumn(\"Text\", regexp_replace(df[\"text\"], \"[^a-z0-9\\\\s]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c4e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some excercises, it didn't work to me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5297c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#header = tweets.first()\n",
    "\n",
    "#tweets = tweets.filter(lambda row: row != header) \n",
    "#tweets = tweets.map(lambda row: [int(elem) for elem in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72ccea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = sc.textFile(\"Tweets_path \")\n",
    "#Header = tweets.first()\n",
    "\n",
    "#tweets = tweets.filter(lambda row: row != header) \n",
    "#tweets = tweets.map(lambda row: [int(elem) for elem in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a618ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the schema for my DataFrame\n",
    "# was one  error because i didn' import pyspark.sql.types as typ\n",
    "# but next did it, the command worked, great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields = [\n",
    "#    *[\n",
    "#        typ.StructField(h[1:-1], typ.IntegerType(), True)\n",
    "#        for h in header.split(',')\n",
    "#    ]\n",
    "#]\n",
    "#schema = typ.StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First put the Titles to see better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d193da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_names  = ['_c0','Ids', 'Date', 'Flag', 'User', 'Text']    \n",
    "#full_df = pd.read_csv('file:///home/hduser/Documents/CA2/ProjectTweets.csv', header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8139bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the file whit  row's title  ******* ******* ****** ******* ****** ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df = \"Title_pro_Tweets.csv\"\n",
    "#Check if the file already Exists\n",
    "#if not os.path.exists(\"Title_pro_Tweets.csv\"):\n",
    "    #perform DataFrame operaction and save the file\n",
    "#   spark_df.write.csv(\"file:///home/hduser/Documents/CA2/Title_pro_Tweets.csv\", header=True)\n",
    "#else:\n",
    "#    print(\"file already exists. No need to save it again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ca73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema = StructType().add(\"_c0\",\"integer\").add(\"Ids\",\"integer\").add(\"Date\",\"string\").add(\"Flag\", \"string\").add(\"User\",'string').add(\"Text\", \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b084ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema for the  DataFrame\n",
    "#schema = StructType([StructField(\"sentence\", StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308bbbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e7a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ab8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
