{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60e3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/johanna23cct/integrated-CA2-MSc-2023094.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0494ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#git remote add origin https://github.com/johanna23cct/integrated-CA2-MSc-2023094.git\n",
    "#git branch -M main\n",
    "#git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac692706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Matplotlib in /usr/lib/python3/dist-packages (3.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990b285b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bokeh in /home/hduser/.local/lib/python3.10/site-packages (3.3.0)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /usr/lib/python3/dist-packages (from bokeh) (3.0.3)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /home/hduser/.local/lib/python3.10/site-packages (from bokeh) (2023.10.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/hduser/.local/lib/python3.10/site-packages (from bokeh) (2.0.3)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/lib/python3/dist-packages (from bokeh) (9.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/lib/python3/dist-packages (from bokeh) (5.4.1)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/lib/python3/dist-packages (from bokeh) (21.3)\n",
      "Requirement already satisfied: tornado>=5.1 in /usr/lib/python3/dist-packages (from bokeh) (6.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/hduser/.local/lib/python3.10/site-packages (from bokeh) (1.24.3)\n",
      "Requirement already satisfied: contourpy>=1 in /home/hduser/.local/lib/python3.10/site-packages (from bokeh) (1.1.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/hduser/.local/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hduser/.local/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.2->bokeh) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2->bokeh) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e2bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: skforecast in /home/hduser/.local/lib/python3.10/site-packages (0.10.1)\n",
      "Requirement already satisfied: pandas<2.1,>=1.2 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (2.0.3)\n",
      "Requirement already satisfied: tqdm<4.66,>=4.57.0 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (4.65.2)\n",
      "Requirement already satisfied: scikit-learn<1.4,>=1.0 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (1.3.0)\n",
      "Requirement already satisfied: joblib<1.4,>=1.1.0 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (1.3.2)\n",
      "Requirement already satisfied: optuna<3.3,>=2.10.0 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (3.2.0)\n",
      "Requirement already satisfied: numpy<1.26,>=1.20 in /home/hduser/.local/lib/python3.10/site-packages (from skforecast) (1.24.3)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from optuna<3.3,>=2.10.0->skforecast) (5.4.1)\n",
      "Requirement already satisfied: colorlog in /home/hduser/.local/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (6.7.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/hduser/.local/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (1.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from optuna<3.3,>=2.10.0->skforecast) (21.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/hduser/.local/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (2.0.22)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in /home/hduser/.local/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas<2.1,>=1.2->skforecast) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hduser/.local/lib/python3.10/site-packages (from pandas<2.1,>=1.2->skforecast) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/hduser/.local/lib/python3.10/site-packages (from pandas<2.1,>=1.2->skforecast) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/hduser/.local/lib/python3.10/site-packages (from scikit-learn<1.4,>=1.0->skforecast) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/lib/python3/dist-packages (from scikit-learn<1.4,>=1.0->skforecast) (1.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/hduser/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna<3.3,>=2.10.0->skforecast) (4.5.0)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna<3.3,>=2.10.0->skforecast) (1.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<2.1,>=1.2->skforecast) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/hduser/.local/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna<3.3,>=2.10.0->skforecast) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install skforecast --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02cb1042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/hduser/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /home/hduser/.local/lib/python3.10/site-packages (from nltk) (4.65.2)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/hduser/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/hduser/.local/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b68e89f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Plots\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "%matplotlib inline\n",
    "\n",
    "# Modeling and Forecasting\n",
    "# ==============================================================================\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\n",
    "#from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "# Spark\n",
    "# import SparkSession library \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lower, concat_ws, regexp_replace\n",
    "from pyspark.sql.functions import split, size, length, broadcast, sum\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField, ArrayType\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "# Warnings configuration\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efeff729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22943cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext \n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce48466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc08cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"data_projectTweets\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb2dc8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema = StructType().add(\"Ids\",\"integer\").add(\"Date\",\"string\").add(\"Flag\", \"string\").add(\"User\",'string').add(\"Text\", \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "217df978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************************************************************************************************************\n",
    "# *                                  Start with the Data                                                         *\n",
    "# ****************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee728360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big headache and colapse, to read the dataset\n",
    "# I was load the file from my hadoop (local)\n",
    "# df = spark.read.csv('home/hduser/Documnets/CA2', header+True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02317436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move the dataset fiel to HDFS path from my terminal:\n",
    "#First check:  $fs -ls/user1  from /home/hduser/Documents/CA2/\n",
    "#Next move :   $hadoop fs-put./ProjectTweets.scv/user1\n",
    "#Check again : $fs -ls/user1\n",
    "#I will use direct from the path, to have one of five V' (Velocity) \n",
    "\n",
    "\n",
    "#folowiong you can see the two way to load the dataset, \n",
    "#    (command)+(file://)+(/path/)+(filename)\n",
    "#I will use direct from the path, to have one of five V' (Velocity) \n",
    "\n",
    "Tweets_path = \"/user1/ProjectTweets.csv\"\n",
    "#df = spark.read.csv(\"file:///home/hduser/Documents/CA2/ProjectTweets.csv\", header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "819163a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/user1/ProjectTweets.csv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tweets_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f67adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df = spark.read.csv(\"file:///home/hduser/Documents/CA2/ProjectTweets.csv\", header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b200bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.csv(\"/user1/ProjectTweets.csv\", header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90864784",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['_c0', 'Ids', 'Date', 'Flag', 'User', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43d21cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, column_name in enumerate(column_names):\n",
    "    spark_df = spark_df.withColumnRenamed(\"_c\" + str(i),column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "084011e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       Ids|                Date|    Flag|           User|                Text|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2db9aba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# display the total number of rows data\n",
    "total_rows = spark_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e283b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 1,600,000\n"
     ]
    }
   ],
   "source": [
    "formatted_total_rows = \"{:,}\".format(total_rows)\n",
    "print(\"Total Rows:\", formatted_total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b74b848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary column  ************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "840bac8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_to_drop = ['Flag', 'Unnamed: 0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e99b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.drop('Flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2fda977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+---------------+--------------------+\n",
      "|_c0|       Ids|                Date|           User|                Text|\n",
      "+---+----------+--------------------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes |\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70531f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a UDF to apply VADER sentiment analysis to a Text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b38b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(Text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment = sid.polarity_scores(Text)\n",
    "    return sentiment['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a97aafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#register the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b98f4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_udf = udf(analyze_sentiment, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "786eb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a43e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"sentiment\", sentiment_udf(spark_df[\"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3311372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+---------------+--------------------+---------+\n",
      "|_c0|       Ids|                Date|           User|                Text|sentiment|\n",
      "+---+----------+--------------------+---------------+--------------------+---------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|  -0.0173|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|    -0.75|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|   0.4939|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|    -0.25|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|  -0.6597|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|      0.0|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |   0.4767|\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|    0.745|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|      0.0|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|      0.0|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|      0.0|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|      0.0|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|  -0.5994|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|  -0.1027|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|   0.3724|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|   0.4545|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|  -0.9081|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes |      0.0|\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|   0.6988|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|   0.1779|\n",
      "+---+----------+--------------------+---------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(spark_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68d7fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'Ids', 'Date', 'Flag', 'User', 'Text']\n"
     ]
    }
   ],
   "source": [
    "#why i have Flag  again??\n",
    "print(column_names[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10198758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+---------------+--------------------+---------+\n",
      "|_c0|       Ids|                Date|           User|                Text|sentiment|\n",
      "+---+----------+--------------------+---------------+--------------------+---------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|  -0.0173|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|    -0.75|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|   0.4939|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|    -0.25|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|  -0.6597|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|      0.0|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |   0.4767|\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|    0.745|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|      0.0|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|      0.0|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|      0.0|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|      0.0|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|  -0.5994|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|  -0.1027|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|   0.3724|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|   0.4545|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|  -0.9081|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes |      0.0|\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|   0.6988|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|   0.1779|\n",
      "+---+----------+--------------------+---------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#to show\n",
    "\n",
    "spark_df.drop('Flag').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fd7df8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+---------------+--------------------+---------+\n",
      "|_c0|       Ids|                Date|           User|                Text|sentiment|\n",
      "+---+----------+--------------------+---------------+--------------------+---------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|  -0.0173|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|    -0.75|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|   0.4939|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|    -0.25|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|  -0.6597|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|      0.0|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |   0.4767|\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|    0.745|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|      0.0|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|      0.0|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|      0.0|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|      0.0|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|  -0.5994|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|  -0.1027|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|   0.3724|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|   0.4545|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|  -0.9081|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes |      0.0|\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|   0.6988|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|   0.1779|\n",
      "+---+----------+--------------------+---------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(spark_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a625ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc43f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating my Dataframe (i did it in line 27 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d79e15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#        Basic Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5502ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ff684aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = Tweets.withColumn(\"word_count\", size(split(Tweets[\"Text\"], \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec1d7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5312f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tutorial did't work because is a Panda code: TypeError: 'Column' object is not callable\n",
    "\n",
    "#Tweets['word_count'] = Tweets['Text'].apply(lambda x: len(str(x).split(\"\")))\n",
    "#Tweets[['Text','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86e96ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                Text|word_count|\n",
      "+--------------------+----------+\n",
      "|@switchfoot http:...|        20|\n",
      "|is upset that he ...|        22|\n",
      "|@Kenichan I dived...|        19|\n",
      "|my whole body fee...|        11|\n",
      "|@nationwideclass ...|        22|\n",
      "|@Kwesidei not the...|         6|\n",
      "|         Need a hug |         4|\n",
      "|@LOLTrish hey  lo...|        24|\n",
      "|@Tatiana_K nope t...|         7|\n",
      "|@twittera que me ...|         6|\n",
      "|spring break in p...|         8|\n",
      "|I just re-pierced...|         6|\n",
      "|@caregiving I cou...|        21|\n",
      "|@octolinz16 It it...|        16|\n",
      "|@smarrison i woul...|        23|\n",
      "|@iamjazzyfizzle I...|        20|\n",
      "|Hollis' death sce...|        19|\n",
      "|about to file taxes |         5|\n",
      "|@LettyA ahh ive a...|        12|\n",
      "|@FakerPattyPattz ...|        13|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tweets.select(\"Text\", \"word_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f000ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44bfbfc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tutorial did't work because is a Panda code:TypeError: 'Column' object is not callable\n",
    "#Tweets['char_count'] = Tweets['Text'].str.len()\n",
    "## this also includes spaces\n",
    "#Tweets[['Text','char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33b87218",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = Tweets.withColumn(\"char_count\", length(Tweets[\"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1bc221ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                Text|char_count|\n",
      "+--------------------+----------+\n",
      "|@switchfoot http:...|       115|\n",
      "|is upset that he ...|       111|\n",
      "|@Kenichan I dived...|        89|\n",
      "|my whole body fee...|        47|\n",
      "|@nationwideclass ...|       111|\n",
      "|@Kwesidei not the...|        29|\n",
      "|         Need a hug |        11|\n",
      "|@LOLTrish hey  lo...|        99|\n",
      "|@Tatiana_K nope t...|        36|\n",
      "|@twittera que me ...|        25|\n",
      "|spring break in p...|        43|\n",
      "|I just re-pierced...|        26|\n",
      "|@caregiving I cou...|        94|\n",
      "|@octolinz16 It it...|        77|\n",
      "|@smarrison i woul...|       117|\n",
      "|@iamjazzyfizzle I...|       103|\n",
      "|Hollis' death sce...|        93|\n",
      "|about to file taxes |        20|\n",
      "|@LettyA ahh ive a...|        64|\n",
      "|@FakerPattyPattz ...|        79|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tweets.select(\"Text\", \"char_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2664f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27f18d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = Tweets.withColumn(\"Text\", concat_ws(\"\", lower(col(\"Text\"))))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "248ab3b6",
   "metadata": {},
   "source": [
    "Tweets.select(\"Text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03d3c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b0ef5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = Tweets.withColumn(\"Text\", regexp_replace(col(\"Text\"), r'[^\\w\\s]', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e490786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Text|\n",
      "+--------------------+\n",
      "|switchfoot httptw...|\n",
      "|is upset that he ...|\n",
      "|kenichan i dived ...|\n",
      "|my whole body fee...|\n",
      "|nationwideclass n...|\n",
      "|kwesidei not the ...|\n",
      "|         need a hug |\n",
      "|loltrish hey  lon...|\n",
      "|tatiana_k nope th...|\n",
      "|twittera que me m...|\n",
      "|spring break in p...|\n",
      "|i just repierced ...|\n",
      "|caregiving i coul...|\n",
      "|octolinz16 it it ...|\n",
      "|smarrison i would...|\n",
      "|iamjazzyfizzle i ...|\n",
      "|hollis death scen...|\n",
      "|about to file taxes |\n",
      "|lettya ahh ive al...|\n",
      "|fakerpattypattz o...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tweets.select(\"Text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02a1e641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+---------------+--------------------+---------+\n",
      "|_c0|       Ids|                Date|           User|                Text|sentiment|\n",
      "+---+----------+--------------------+---------------+--------------------+---------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|  -0.0173|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|    -0.75|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|   0.4939|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|    -0.25|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|  -0.6597|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|      0.0|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |   0.4767|\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|    0.745|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|      0.0|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|      0.0|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|      0.0|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|      0.0|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|  -0.5994|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|  -0.1027|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|   0.3724|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|   0.4545|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|  -0.9081|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes |      0.0|\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|   0.6988|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|   0.1779|\n",
      "+---+----------+--------------------+---------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.show()    #Why no apear all new columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56a747e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count stop Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a22a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"StopwordsCount\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6872f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why i must to read again the dataset, if i did it in line [40]\n",
    "Tweets = spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9c07085",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"is\", \"the\", \"I\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "539f3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stopwords(text):\n",
    "    words = text.split()\n",
    "    return len([word for word in words if word in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7cb76ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_stopwords_udf = udf(count_stopwords, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68b5a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = Tweets.withColumn(\"Stopwords\", count_stopwords_udf(col(\"Text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2dde6366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+\n",
      "|                Text|Sentiment|Stopwords|\n",
      "+--------------------+---------+---------+\n",
      "|@switchfoot http:...|  -0.0173|        0|\n",
      "|is upset that he ...|    -0.75|        1|\n",
      "|@Kenichan I dived...|   0.4939|        2|\n",
      "|my whole body fee...|    -0.25|        0|\n",
      "|@nationwideclass ...|  -0.6597|        1|\n",
      "|@Kwesidei not the...|      0.0|        1|\n",
      "|         Need a hug |   0.4767|        0|\n",
      "|@LOLTrish hey  lo...|    0.745|        0|\n",
      "|@Tatiana_K nope t...|      0.0|        0|\n",
      "|@twittera que me ...|      0.0|        0|\n",
      "|spring break in p...|      0.0|        0|\n",
      "|I just re-pierced...|      0.0|        1|\n",
      "|@caregiving I cou...|  -0.5994|        3|\n",
      "|@octolinz16 It it...|  -0.1027|        1|\n",
      "|@smarrison i woul...|   0.3724|        1|\n",
      "|@iamjazzyfizzle I...|   0.4545|        4|\n",
      "|Hollis' death sce...|  -0.9081|        1|\n",
      "|about to file taxes |      0.0|        0|\n",
      "|@LettyA ahh ive a...|   0.6988|        1|\n",
      "|@FakerPattyPattz ...|   0.1779|        1|\n",
      "+--------------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Tweets.select(\"Text\", \"Sentiment\", \"Stopwords\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a865830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number  of Special characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b1e6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SpecialCharactersCount\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "44bc4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "270fe5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Special_Characters = [\"@\", \"#\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b26bcd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_Special_Characters(text):\n",
    "    count = 0\n",
    "    for char in Special_Characters:\n",
    "        count += text.count(char)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c61ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_Special_Characters_udf = udf(count_Special_Characters, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9bb7ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = Tweets.withColumn(\"Special_Characters_count\", count_Special_Characters_udf(col(\"Text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aa22f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = Tweets.select(\"Text\", \"Special_Characters_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "80ea2cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+\n",
      "|                Text|Special_Characters_count|\n",
      "+--------------------+------------------------+\n",
      "|@switchfoot http:...|                       1|\n",
      "|is upset that he ...|                       0|\n",
      "|@Kenichan I dived...|                       1|\n",
      "|my whole body fee...|                       0|\n",
      "|@nationwideclass ...|                       1|\n",
      "|@Kwesidei not the...|                       1|\n",
      "|         Need a hug |                       0|\n",
      "|@LOLTrish hey  lo...|                       1|\n",
      "|@Tatiana_K nope t...|                       1|\n",
      "|@twittera que me ...|                       1|\n",
      "|spring break in p...|                       0|\n",
      "|I just re-pierced...|                       0|\n",
      "|@caregiving I cou...|                       1|\n",
      "|@octolinz16 It it...|                       1|\n",
      "|@smarrison i woul...|                       1|\n",
      "|@iamjazzyfizzle I...|                       2|\n",
      "|Hollis' death sce...|                       0|\n",
      "|about to file taxes |                       0|\n",
      "|@LettyA ahh ive a...|                       1|\n",
      "|@FakerPattyPattz ...|                       1|\n",
      "+--------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8398f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages Word ** Spark session **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "883d5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c6a12a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"AverageWordLength\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e61f0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d6df95da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d1d0cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_udf = udf(avg_word, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5f9c9bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = Tweets.withColumn(\"average_word_length\", avg_word_udf(col(\"Text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9a06e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_columns = Tweets.select(\"Text\", \"average_word_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7ca27493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7348fbf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o388.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1522)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:122)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:177)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:426)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:417)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:504)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:88)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute$(EvalPythonExec.scala:87)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.doExecute(BatchEvalPythonExec.scala:34)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:526)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:454)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:497)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:750)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:326)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:445)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12137/1238748644.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o388.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1522)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:122)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:177)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:426)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:417)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:504)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:88)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute$(EvalPythonExec.scala:87)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.doExecute(BatchEvalPythonExec.scala:34)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:526)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:454)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:497)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:750)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:326)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:445)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "Tweets.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bb933856",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0340256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9fb53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395115dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c77711f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e77d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ae182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a508d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"AverageWordLength\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_df = Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_df = spark.createDataFrame(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2580bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_udf = udf(avg_word, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_df  = Tweets_df .withColumn(\"average_word_length\", avg_word_udf(col(\"_6\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dcba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287ebd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema for the  DataFrame\n",
    "#schema = StructType([StructField(\"sentence\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e609e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Crate a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f36b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = [(\"Text\",)]\n",
    "#df = spark.createDataFrame(Tweets, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185526a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df =df.withColumn(\"words\", split(col(\"sentence\"), \" \"))\n",
    "#df =df.withColumn(\"words_lengths\", size(col(\"words\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8054f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul the average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_word_length = df.select(sum(col(\"word_lengths\")).alias(\"total_length\")).collect()[0][\"total_length\"] / df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Average word length: {avg_word_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439de19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"AverageWordLength\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e630c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616652b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_udf = udf(avg_word, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d5c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"average_word_length\", avg_word_udf(col(\"Text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5659da",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df [\"avg_word\"] = spark_df [\"Text\"].apply(lambda x : avg_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03810dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55626e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4cf802",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_udf = udf(lambda sentence: avg_word(sentence), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879dfbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = Tweets.withColumn(\"average_word_length\", avg_word_udf(col(\"Text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f392a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d68076a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b472d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e95ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c07ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafc41c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679e422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e22b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_udf = udf(avg_word, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62404be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = Tweets.withColumn(\"average_word_length\", avg_word_udf(col(\"Text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets[\"avg_word\"] = Tweets[\"Text\"].apply(lambda x : avg_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ba154",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets.select(\"Text\").show()  #.show() Error   #it isn't show the \"avg_word_udf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bec2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets[\"avg_word\"]= Tweets[\"Text\"].apply(lambda x : avg_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7700dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets[['Text', 'avg_word']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a063ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd965a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd407b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855bcd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec531d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af8b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f47f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f2eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22a48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9af6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd635b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "79d38b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************************************************************************\n",
    "#*                                       T O K E N I Z A T I O N                                                 *\n",
    "#*****************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "71ef43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "77667d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_Tweets(Text):\n",
    "    blob = TextBlob(Text)\n",
    "    return [str(word) for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "76bf1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_udf = udf(tokenize_Tweets, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9b9275b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8d1f7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn('tokenized_Tweets', tokenize_udf(spark_df['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fe2d18f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                Text|    tokenized_Tweets|\n",
      "+--------------------+--------------------+\n",
      "|@switchfoot http:...|[switchfoot, http...|\n",
      "|is upset that he ...|[is, upset, that,...|\n",
      "|@Kenichan I dived...|[Kenichan, I, div...|\n",
      "|my whole body fee...|[my, whole, body,...|\n",
      "|@nationwideclass ...|[nationwideclass,...|\n",
      "|@Kwesidei not the...|[Kwesidei, not, t...|\n",
      "|         Need a hug |      [Need, a, hug]|\n",
      "|@LOLTrish hey  lo...|[LOLTrish, hey, l...|\n",
      "|@Tatiana_K nope t...|[Tatiana_K, nope,...|\n",
      "|@twittera que me ...|[twittera, que, m...|\n",
      "|spring break in p...|[spring, break, i...|\n",
      "|I just re-pierced...|[I, just, re-pier...|\n",
      "|@caregiving I cou...|[caregiving, I, c...|\n",
      "|@octolinz16 It it...|[octolinz16, It, ...|\n",
      "|@smarrison i woul...|[smarrison, i, wo...|\n",
      "|@iamjazzyfizzle I...|[iamjazzyfizzle, ...|\n",
      "|Hollis' death sce...|[Hollis, death, s...|\n",
      "|about to file taxes |[about, to, file,...|\n",
      "|@LettyA ahh ive a...|[LettyA, ahh, ive...|\n",
      "|@FakerPattyPattz ...|[FakerPattyPattz,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df[['Text', 'tokenized_Tweets']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fe8b1124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_columns = Tweets.select(\"Text\", \"tokenized_Tweets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90246c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************************************************************************************\n",
    "#*                                        S T R E M M I N G                                                     *\n",
    "#****************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a7c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Udf for Stremming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    words = nltk.word_tokenize(Text)\n",
    "    stemmer_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ff488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb79476",
   "metadata": {},
   "outputs": [],
   "source": [
    "strem_udf = udf(stem_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382451d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113bec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"StemmingExample\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fca207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_stemmed = spark_df.withColumn(\"Stemmed_Text\", stem_udf(spark_df[\"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show te resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f840a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_stemmed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a08e036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382efd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d758ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14644c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Procesising in panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.withColumn(\"Text\", lower(df[\"text\"]))\n",
    "#df = df.withColumn(\"Text\", regexp_replace(df[\"text\"], \"[^a-z0-9\\\\s]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42db868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4de8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1276b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c2580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ff6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6680cd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ff69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Text\", lower(df[\"text\"]))\n",
    "df = df.withColumn(\"Text\", regexp_replace(df[\"text\"], \"[^a-z0-9\\\\s]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c4e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some excercises, it didn't work to me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5297c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#header = tweets.first()\n",
    "\n",
    "#tweets = tweets.filter(lambda row: row != header) \n",
    "#tweets = tweets.map(lambda row: [int(elem) for elem in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72ccea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = sc.textFile(\"Tweets_path \")\n",
    "#Header = tweets.first()\n",
    "\n",
    "#tweets = tweets.filter(lambda row: row != header) \n",
    "#tweets = tweets.map(lambda row: [int(elem) for elem in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a618ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the schema for my DataFrame\n",
    "# was one  error because i didn' import pyspark.sql.types as typ\n",
    "# but next did it, the command worked, great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields = [\n",
    "#    *[\n",
    "#        typ.StructField(h[1:-1], typ.IntegerType(), True)\n",
    "#        for h in header.split(',')\n",
    "#    ]\n",
    "#]\n",
    "#schema = typ.StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First put the Titles to see better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d193da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_names  = ['_c0','Ids', 'Date', 'Flag', 'User', 'Text']    \n",
    "#full_df = pd.read_csv('file:///home/hduser/Documents/CA2/ProjectTweets.csv', header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8139bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the file whit  row's title  ******* ******* ****** ******* ****** ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df = \"Title_pro_Tweets.csv\"\n",
    "#Check if the file already Exists\n",
    "#if not os.path.exists(\"Title_pro_Tweets.csv\"):\n",
    "    #perform DataFrame operaction and save the file\n",
    "#   spark_df.write.csv(\"file:///home/hduser/Documents/CA2/Title_pro_Tweets.csv\", header=True)\n",
    "#else:\n",
    "#    print(\"file already exists. No need to save it again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ca73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema = StructType().add(\"_c0\",\"integer\").add(\"Ids\",\"integer\").add(\"Date\",\"string\").add(\"Flag\", \"string\").add(\"User\",'string').add(\"Text\", \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b084ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema for the  DataFrame\n",
    "#schema = StructType([StructField(\"sentence\", StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308bbbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e7a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ab8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
